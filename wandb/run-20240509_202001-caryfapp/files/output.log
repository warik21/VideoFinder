c:\Users\eriki\OneDrive\Documents\all_folder\other_projects\VideoFinder\VFenv\Lib\site-packages\transformers\models\gemma\modeling_gemma.py:573: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\aten\src\ATen\native\transformers\cuda\sdp_utils.cpp:263.)
  attn_output = torch.nn.functional.scaled_dot_product_attention(
Total params: 2506.17M, Trainable: 744.50M
Total params: 2506.17M, Trainable: 524.29M
Total params: 2506.17M, Trainable: 524.29M
Total params: 2506.17M, Trainable: 524.29M
Total params: 2506.17M, Trainable: 524.29M
Total params: 2506.17M, Trainable: 524.29M
Total params: 2506.17M, Trainable: 744.50M