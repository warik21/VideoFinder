{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16dfec1a756d4bdc93eff8b91a13b42b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"google/gemma-1.1-2b-it\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e7a00bc21c450dbcd5b018890b5ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_df = pd.read_csv('csvs/videos_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_description</th>\n",
       "      <th>video_transcript</th>\n",
       "      <th>video_name</th>\n",
       "      <th>channel_name</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_url</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>https://www.youtube.com/watch?v=Z_EliVUkuFA</th>\n",
       "      <td>Summary extraction error: Unexpected response ...</td>\n",
       "      <td>The video discusses the potential of Google De...</td>\n",
       "      <td>DeepMind‚Äôs New AI: Assistant From The Future!</td>\n",
       "      <td>Two Minute Papers</td>\n",
       "      <td>[\"A video about the limitations of Google Deep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.youtube.com/watch?v=_2bzwNyIjkY</th>\n",
       "      <td>The video provides information about Andrew Pr...</td>\n",
       "      <td>The video highlights the many improvements in ...</td>\n",
       "      <td>Blender 4.1 - Create Virtual Worlds‚Ä¶For Free!</td>\n",
       "      <td>Two Minute Papers</td>\n",
       "      <td>['A video about the split viewer node in Blend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.youtube.com/watch?v=1YEX4t79e0Q</th>\n",
       "      <td>Summary extraction error: Unexpected response ...</td>\n",
       "      <td>OpenAI‚Äôs text to video AI, Sora took the world...</td>\n",
       "      <td>OpenAI Sora: Beauty And Horror!</td>\n",
       "      <td>Two Minute Papers</td>\n",
       "      <td>[\"A video about the AI's ability to create abs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.youtube.com/watch?v=IS0xphCc5rI</th>\n",
       "      <td>The conference \"Fully Connected\" is about expl...</td>\n",
       "      <td>OpenAI‚Äôs Sora is a brilliant new text-to-video...</td>\n",
       "      <td>OpenAI Sora Just Supercharged Filmmaking!</td>\n",
       "      <td>Two Minute Papers</td>\n",
       "      <td>['A video about exploring the intersection bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.youtube.com/watch?v=Y9cwnHor8es</th>\n",
       "      <td>The video provides information about a paper o...</td>\n",
       "      <td>The video highlights the advancements in artif...</td>\n",
       "      <td>NVIDIA GTC: This Is The Future Of Everything!</td>\n",
       "      <td>Two Minute Papers</td>\n",
       "      <td>['A video about the potential impact of artifi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             video_description  \\\n",
       "video_url                                                                                        \n",
       "https://www.youtube.com/watch?v=Z_EliVUkuFA  Summary extraction error: Unexpected response ...   \n",
       "https://www.youtube.com/watch?v=_2bzwNyIjkY  The video provides information about Andrew Pr...   \n",
       "https://www.youtube.com/watch?v=1YEX4t79e0Q  Summary extraction error: Unexpected response ...   \n",
       "https://www.youtube.com/watch?v=IS0xphCc5rI  The conference \"Fully Connected\" is about expl...   \n",
       "https://www.youtube.com/watch?v=Y9cwnHor8es  The video provides information about a paper o...   \n",
       "\n",
       "                                                                              video_transcript  \\\n",
       "video_url                                                                                        \n",
       "https://www.youtube.com/watch?v=Z_EliVUkuFA  The video discusses the potential of Google De...   \n",
       "https://www.youtube.com/watch?v=_2bzwNyIjkY  The video highlights the many improvements in ...   \n",
       "https://www.youtube.com/watch?v=1YEX4t79e0Q  OpenAI‚Äôs text to video AI, Sora took the world...   \n",
       "https://www.youtube.com/watch?v=IS0xphCc5rI  OpenAI‚Äôs Sora is a brilliant new text-to-video...   \n",
       "https://www.youtube.com/watch?v=Y9cwnHor8es  The video highlights the advancements in artif...   \n",
       "\n",
       "                                                                                video_name  \\\n",
       "video_url                                                                                    \n",
       "https://www.youtube.com/watch?v=Z_EliVUkuFA  DeepMind‚Äôs New AI: Assistant From The Future!   \n",
       "https://www.youtube.com/watch?v=_2bzwNyIjkY  Blender 4.1 - Create Virtual Worlds‚Ä¶For Free!   \n",
       "https://www.youtube.com/watch?v=1YEX4t79e0Q                OpenAI Sora: Beauty And Horror!   \n",
       "https://www.youtube.com/watch?v=IS0xphCc5rI      OpenAI Sora Just Supercharged Filmmaking!   \n",
       "https://www.youtube.com/watch?v=Y9cwnHor8es  NVIDIA GTC: This Is The Future Of Everything!   \n",
       "\n",
       "                                                  channel_name  \\\n",
       "video_url                                                        \n",
       "https://www.youtube.com/watch?v=Z_EliVUkuFA  Two Minute Papers   \n",
       "https://www.youtube.com/watch?v=_2bzwNyIjkY  Two Minute Papers   \n",
       "https://www.youtube.com/watch?v=1YEX4t79e0Q  Two Minute Papers   \n",
       "https://www.youtube.com/watch?v=IS0xphCc5rI  Two Minute Papers   \n",
       "https://www.youtube.com/watch?v=Y9cwnHor8es  Two Minute Papers   \n",
       "\n",
       "                                                                                        prompt  \n",
       "video_url                                                                                       \n",
       "https://www.youtube.com/watch?v=Z_EliVUkuFA  [\"A video about the limitations of Google Deep...  \n",
       "https://www.youtube.com/watch?v=_2bzwNyIjkY  ['A video about the split viewer node in Blend...  \n",
       "https://www.youtube.com/watch?v=1YEX4t79e0Q  [\"A video about the AI's ability to create abs...  \n",
       "https://www.youtube.com/watch?v=IS0xphCc5rI  ['A video about exploring the intersection bet...  \n",
       "https://www.youtube.com/watch?v=Y9cwnHor8es  ['A video about the potential impact of artifi...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = videos_df[['video_description', 'video_name', 'prompt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_description</th>\n",
       "      <th>video_name</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_url</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>https://www.youtube.com/watch?v=Z_EliVUkuFA</th>\n",
       "      <td>Summary extraction error: Unexpected response ...</td>\n",
       "      <td>DeepMind‚Äôs New AI: Assistant From The Future!</td>\n",
       "      <td>[\"A video about the limitations of Google Deep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.youtube.com/watch?v=_2bzwNyIjkY</th>\n",
       "      <td>The video provides information about Andrew Pr...</td>\n",
       "      <td>Blender 4.1 - Create Virtual Worlds‚Ä¶For Free!</td>\n",
       "      <td>['A video about the split viewer node in Blend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.youtube.com/watch?v=1YEX4t79e0Q</th>\n",
       "      <td>Summary extraction error: Unexpected response ...</td>\n",
       "      <td>OpenAI Sora: Beauty And Horror!</td>\n",
       "      <td>[\"A video about the AI's ability to create abs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.youtube.com/watch?v=IS0xphCc5rI</th>\n",
       "      <td>The conference \"Fully Connected\" is about expl...</td>\n",
       "      <td>OpenAI Sora Just Supercharged Filmmaking!</td>\n",
       "      <td>['A video about exploring the intersection bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.youtube.com/watch?v=Y9cwnHor8es</th>\n",
       "      <td>The video provides information about a paper o...</td>\n",
       "      <td>NVIDIA GTC: This Is The Future Of Everything!</td>\n",
       "      <td>['A video about the potential impact of artifi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             video_description  \\\n",
       "video_url                                                                                        \n",
       "https://www.youtube.com/watch?v=Z_EliVUkuFA  Summary extraction error: Unexpected response ...   \n",
       "https://www.youtube.com/watch?v=_2bzwNyIjkY  The video provides information about Andrew Pr...   \n",
       "https://www.youtube.com/watch?v=1YEX4t79e0Q  Summary extraction error: Unexpected response ...   \n",
       "https://www.youtube.com/watch?v=IS0xphCc5rI  The conference \"Fully Connected\" is about expl...   \n",
       "https://www.youtube.com/watch?v=Y9cwnHor8es  The video provides information about a paper o...   \n",
       "\n",
       "                                                                                video_name  \\\n",
       "video_url                                                                                    \n",
       "https://www.youtube.com/watch?v=Z_EliVUkuFA  DeepMind‚Äôs New AI: Assistant From The Future!   \n",
       "https://www.youtube.com/watch?v=_2bzwNyIjkY  Blender 4.1 - Create Virtual Worlds‚Ä¶For Free!   \n",
       "https://www.youtube.com/watch?v=1YEX4t79e0Q                OpenAI Sora: Beauty And Horror!   \n",
       "https://www.youtube.com/watch?v=IS0xphCc5rI      OpenAI Sora Just Supercharged Filmmaking!   \n",
       "https://www.youtube.com/watch?v=Y9cwnHor8es  NVIDIA GTC: This Is The Future Of Everything!   \n",
       "\n",
       "                                                                                        prompt  \n",
       "video_url                                                                                       \n",
       "https://www.youtube.com/watch?v=Z_EliVUkuFA  [\"A video about the limitations of Google Deep...  \n",
       "https://www.youtube.com/watch?v=_2bzwNyIjkY  ['A video about the split viewer node in Blend...  \n",
       "https://www.youtube.com/watch?v=1YEX4t79e0Q  [\"A video about the AI's ability to create abs...  \n",
       "https://www.youtube.com/watch?v=IS0xphCc5rI  ['A video about exploring the intersection bet...  \n",
       "https://www.youtube.com/watch?v=Y9cwnHor8es  ['A video about the potential impact of artifi...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look into the first row of the dataset\n",
    "row = dataset.iloc[1].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video_description': \"The video provides information about Andrew Price's donut tutorial and a paper on simulations that look almost like reality. The video also mentions the Patreon supporters who make Two Minute Papers possible.\",\n",
       " 'video_name': 'Blender 4.1 - Create Virtual Worlds‚Ä¶For Free!',\n",
       " 'prompt': \"['A video about the split viewer node in Blender 4.1.']\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_row(row):\n",
    "    return (f\"Below is a video title and description summary. \" \\\n",
    "            f\"Write a prompt that can be used to adress the video description. \" \\\n",
    "            f\"### Description:\\n{row['video_description']}\\n\\n ### title:\\n{row['video_name']}\").format_map(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "row2 = prompt_row(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Below is a video title and description summary. Write a prompt that can be used to adress the video description. ### Description:\\nThe video provides information about Andrew Price's donut tutorial and a paper on simulations that look almost like reality. The video also mentions the Patreon supporters who make Two Minute Papers possible.\\n\\n ### title:\\nBlender 4.1 - Create Virtual Worlds‚Ä¶For Free!\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [prompt_row(row) for _, row in dataset.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "876"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = \"</s>\"\n",
    "outputs = [row['prompt'] + EOS_TOKEN for _, row in dataset.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"A video about the limitations of Google DeepMind\\'s Gemini 1.5 Pro AI in\"]</s>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [{\"prompt\":s, \"output\":t, \"example\": s+t} for s,t in zip(prompts, outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Below is a video title and description summary. Write a prompt that can be used to adress the video description. ### Description:\\nSummary extraction error: Unexpected response format.\\n\\n ### title:\\nDeepMind‚Äôs New AI: Assistant From The Future!',\n",
       " 'output': '[\"A video about the limitations of Google DeepMind\\'s Gemini 1.5 Pro AI in\"]</s>',\n",
       " 'example': 'Below is a video title and description summary. Write a prompt that can be used to adress the video description. ### Description:\\nSummary extraction error: Unexpected response format.\\n\\n ### title:\\nDeepMind‚Äôs New AI: Assistant From The Future![\"A video about the limitations of Google DeepMind\\'s Gemini 1.5 Pro AI in\"]</s>'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'google/gemma-1.1-2b-it'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2926, 13818, 708, 2319, 3779, 235341]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"My experiments are going strong!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merikice1\u001b[0m (\u001b[33mwarik21\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\eriki\\OneDrive\\Documents\\all_folder\\other_projects\\VideoFinder\\wandb\\run-20240507_180758-l0xmhn32</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/warik21/VideoFinder/runs/l0xmhn32' target=\"_blank\">winter-galaxy-6</a></strong> to <a href='https://wandb.ai/warik21/VideoFinder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/warik21/VideoFinder' target=\"_blank\">https://wandb.ai/warik21/VideoFinder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/warik21/VideoFinder/runs/l0xmhn32' target=\"_blank\">https://wandb.ai/warik21/VideoFinder/runs/l0xmhn32</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d4b2fdef6b9420d98f38ab1391d3237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.005 MB uploaded\\r'), FloatProgress(value=0.2654494382022472, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">winter-galaxy-6</strong> at: <a href='https://wandb.ai/warik21/VideoFinder/runs/l0xmhn32' target=\"_blank\">https://wandb.ai/warik21/VideoFinder/runs/l0xmhn32</a><br/> View project at: <a href='https://wandb.ai/warik21/VideoFinder' target=\"_blank\">https://wandb.ai/warik21/VideoFinder</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240507_180758-l0xmhn32\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with wandb.init(project=\"VideoFinder\"):\n",
    "    at = wandb.Artifact(\n",
    "        name=\"video_prompts\",\n",
    "        type=\"dataset\",\n",
    "        description=\"Prompts for video descriptions\",\n",
    "    )\n",
    "    at.add_file('csvs/videos_df.csv')\n",
    "\n",
    "    table = wandb.Table(data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc854dd99a274c4a8a622b967befc35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\eriki\\OneDrive\\Documents\\all_folder\\other_projects\\VideoFinder\\wandb\\run-20240507_180804-n0hyj8nk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/warik21/VideoFinder/runs/n0hyj8nk' target=\"_blank\">absurd-dust-7</a></strong> to <a href='https://wandb.ai/warik21/VideoFinder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/warik21/VideoFinder' target=\"_blank\">https://wandb.ai/warik21/VideoFinder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/warik21/VideoFinder/runs/n0hyj8nk' target=\"_blank\">https://wandb.ai/warik21/VideoFinder/runs/n0hyj8nk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aeb2736d18a416ba1f7baf726285136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='2.189 MB of 2.189 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">absurd-dust-7</strong> at: <a href='https://wandb.ai/warik21/VideoFinder/runs/n0hyj8nk' target=\"_blank\">https://wandb.ai/warik21/VideoFinder/runs/n0hyj8nk</a><br/> View project at: <a href='https://wandb.ai/warik21/VideoFinder' target=\"_blank\">https://wandb.ai/warik21/VideoFinder</a><br/>Synced 4 W&B file(s), 2 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240507_180804-n0hyj8nk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_dataset = dataset[:800]\n",
    "eval_dataset = dataset[800:]\n",
    "\n",
    "train_table = wandb.Table(dataframe=pd.DataFrame(train_dataset))\n",
    "eval_table  = wandb.Table(dataframe=pd.DataFrame(eval_dataset))\n",
    "\n",
    "\n",
    "with wandb.init(project=\"VideoFinder\", job_type=\"split_data\"):\n",
    "    wandb.log({\"train_dataset\":train_table, \"eval_dataset\":eval_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack(dataset, max_seq_len=1024):\n",
    "    tkds_ids = tokenizer([s[\"example\"] for s in dataset])[\"input_ids\"]\n",
    "    \n",
    "    all_token_ids = []\n",
    "    for tokenized_input in tkds_ids:\n",
    "        all_token_ids.extend(tokenized_input + [tokenizer.eos_token_id])\n",
    "    \n",
    "    packed_ds = []\n",
    "    for i in range(0, len(all_token_ids), max_seq_len+1):\n",
    "        input_ids = all_token_ids[i : i + max_seq_len+1]\n",
    "        if len(input_ids) == (max_seq_len+1):\n",
    "            packed_ds.append({\"input_ids\": input_ids[:-1], \"labels\": input_ids[1:]})  # < --- ‚ÄºÔ∏è ‚õîÔ∏è\n",
    "\t    # if you use the model.output.loss you don't need to shift, it is done for you!\n",
    "    return packed_ds\n",
    "\n",
    "\n",
    "train_ds_packed = pack(train_dataset)\n",
    "eval_ds_packed = pack(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_jsonl(data, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for entry in data:\n",
    "            json.dump(entry, file)\n",
    "            file.write('\\n')\n",
    "\n",
    "\n",
    "# dump everything to jsonl files\n",
    "save_jsonl(train_ds_packed, \"train_packed_VF.jsonl\")\n",
    "save_jsonl(eval_ds_packed, \"eval_packed_VF.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\eriki\\OneDrive\\Documents\\all_folder\\other_projects\\VideoFinder\\wandb\\run-20240507_180819-1cti5v9s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/warik21/VideoFinder/runs/1cti5v9s' target=\"_blank\">wise-cloud-8</a></strong> to <a href='https://wandb.ai/warik21/VideoFinder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/warik21/VideoFinder' target=\"_blank\">https://wandb.ai/warik21/VideoFinder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/warik21/VideoFinder/runs/1cti5v9s' target=\"_blank\">https://wandb.ai/warik21/VideoFinder/runs/1cti5v9s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a89f5452f354db795adcb9dcc68aa99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='1.331 MB of 1.331 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wise-cloud-8</strong> at: <a href='https://wandb.ai/warik21/VideoFinder/runs/1cti5v9s' target=\"_blank\">https://wandb.ai/warik21/VideoFinder/runs/1cti5v9s</a><br/> View project at: <a href='https://wandb.ai/warik21/VideoFinder' target=\"_blank\">https://wandb.ai/warik21/VideoFinder</a><br/>Synced 4 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240507_180819-1cti5v9s\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "packed_at = wandb.Artifact(\n",
    "    name=\"packed_VideoFinder\",\n",
    "    type=\"dataset\",\n",
    "    description=\"VideoFinder dataset packed in sequences\",\n",
    "    metadata={\"max_seq_len\":1024, \"model_id\":model_id})\n",
    "\n",
    "packed_at.add_file(\"train_packed_VF.jsonl\")\n",
    "packed_at.add_file(\"eval_packed_VF.jsonl\")\n",
    "\n",
    "# log the artifact to the project, we can give this run a job_type like `preprocess`\n",
    "with wandb.init(project=\"VideoFinder\", job_type=\"preprocess\"):\n",
    "    wandb.log_artifact(packed_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6236cd81804dd992369225c39da766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011277777776639495, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\eriki\\OneDrive\\Documents\\all_folder\\other_projects\\VideoFinder\\wandb\\run-20240507_180828-6u6y1cz1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/warik21/VideoFinder/runs/6u6y1cz1' target=\"_blank\">sleek-star-9</a></strong> to <a href='https://wandb.ai/warik21/VideoFinder' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/warik21/VideoFinder' target=\"_blank\">https://wandb.ai/warik21/VideoFinder</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/warik21/VideoFinder/runs/6u6y1cz1' target=\"_blank\">https://wandb.ai/warik21/VideoFinder/runs/6u6y1cz1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "run = wandb.init(project=\"VideoFinder\", job_type=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "artifact = run.use_artifact(\"packed_VideoFinder:latest\", type=\"dataset\")\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\eriki\\\\OneDrive\\\\Documents\\\\all_folder\\\\other_projects\\\\VideoFinder\\\\artifacts\\\\packed_VideoFinder-v1'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = artifact.metadata[\"max_seq_len\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator, # we don't need any special collator üòé\n",
    ")\n",
    "\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_ds_packed,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator,\n",
    "    shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['input_ids', 'labels']),\n",
       " tensor([     2,  33501,    603,    476,   3569,   4680,    578,   5966,  13367,\n",
       "         235265,  15615,    476,  18335,    674,    798,    614,   1671,    577,\n",
       "          75147,    573,   3569,   5966, 235265,  43774,   8290]),\n",
       " tensor([ 33501,    603,    476,   3569,   4680,    578,   5966,  13367, 235265,\n",
       "          15615,    476,  18335,    674,    798,    614,   1671,    577,  75147,\n",
       "            573,   3569,   5966, 235265,  43774,   8290, 235292]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = next(iter(train_dataloader))\n",
    "b.keys(), b[\"input_ids\"][0][:25], b[\"labels\"][0][:25]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "gradient_accumulation_steps = 32 // batch_size\n",
    "\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    model_id=model_id,\n",
    "    dataset_name=\"VideoFinder\",\n",
    "    precision=\"bf16\",  # faster and better than fp16, requires new GPUs\n",
    "    n_freeze=8,  # How many layers we don't train, LLama 7B has 32. #TODO : check the number of layers\n",
    "    lr=2e-4,\n",
    "    n_eval_samples=10, # How many samples to generate on validation\n",
    "    max_seq_len=max_seq_len, # Length of the sequences to pack\n",
    "    epochs=3,  # we do 3 pasess over the dataset.\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,  # evey how many iterations we update the gradients, simulates larger batch sizes\n",
    "    batch_size=batch_size,  # what my GPU can handle, depends on how many layers are we training  \n",
    "    log_model=True,  # upload the model to W&B?\n",
    "    mom=0.9, # optim param\n",
    "    gradient_checkpointing = True,  # saves even more memory\n",
    "    freeze_embed = True,  # why train this? let's keep them frozen ‚ùÑÔ∏è\n",
    ")\n",
    "\n",
    "\n",
    "config.total_train_steps = config.epochs * len(train_dataloader) // config.gradient_accumulation_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608a3c2a2d604a5094ad9a83229764b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_id,\n",
    "    device_map=0,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 2506.17M, Trainable: 2506.17M\n"
     ]
    }
   ],
   "source": [
    "def param_count(m):\n",
    "    params = sum([p.numel() for p in m.parameters()])/1_000_000\n",
    "    trainable_params = sum([p.numel() for p in m.parameters() if p.requires_grad])/1_000_000\n",
    "    print(f\"Total params: {params:.2f}M, Trainable: {trainable_params:.2f}M\")\n",
    "    return params, trainable_params\n",
    "\n",
    "params, trainable_params = param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_freeze = 8 # you can play with this parameter\n",
    "\n",
    "\n",
    "# freeze layers (disable gradients)\n",
    "for param in model.parameters(): param.requires_grad = False\n",
    "for param in model.lm_head.parameters(): param.requires_grad = True\n",
    "for param in model.model.layers[n_freeze:].parameters(): param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=config.lr, betas=(0.9,0.99), eps=1e-5)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optim,\n",
    "    num_training_steps=config.total_train_steps,\n",
    "    num_warmup_steps=config.total_train_steps // 10,\n",
    ")\n",
    "\n",
    "\n",
    "def loss_fn(x, y):\n",
    "    \"A Flat CrossEntropy\" \n",
    "    return torch.nn.functional.cross_entropy(x.view(-1, x.shape[-1]), y.view(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "gen_config = GenerationConfig.from_pretrained(config.model_id)\n",
    "test_config = SimpleNamespace(\n",
    "    max_new_tokens=256,\n",
    "    gen_config=gen_config)\n",
    "\n",
    "\n",
    "def generate(prompt, max_new_tokens=100, gen_config=gen_config):\n",
    "    with torch.inference_mode():\n",
    "        tokenized_prompt = tokenizer(prompt, return_tensors='pt')['input_ids'].cuda()\n",
    "        output = model.generate(tokenized_prompt, \n",
    "                            max_new_tokens=max_new_tokens, \n",
    "                            generation_config=gen_config)\n",
    "    return tokenizer.decode(output[0][len(tokenized_prompt[0]):], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def prompt_table(prompts, log=True):\n",
    "    table = wandb.Table(columns=[\"prompt\", \"generation\", \"concat\", \"max_new_tokens\", \"temperature\", \"top_p\"])\n",
    "    for prompt in tqdm(prompts):\n",
    "        out = generate(prompt, test_config.max_new_tokens, test_config.gen_config)\n",
    "        table.add_data(prompt, out, prompt+out, test_config.max_new_tokens, test_config.gen_config.temperature, test_config.gen_config.top_p)\n",
    "    if log:\n",
    "        wandb.log({\"predictions\":table})\n",
    "    return table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_gpu(tensor_dict):\n",
    "    return {k: v.to('cuda') for k, v in tensor_dict.items()}\n",
    "\n",
    "class Accuracy:\n",
    "    \"A simple Accuracy function compatible with HF models\"\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.tp = 0.\n",
    "    def update(self, logits, labels):\n",
    "        logits, labels = logits.argmax(dim=-1).view(-1).cpu(), labels.view(-1).cpu()\n",
    "        tp = (logits == labels).sum()\n",
    "        self.count += len(logits)\n",
    "        self.tp += tp\n",
    "        return tp / len(logits)\n",
    "    def compute(self):\n",
    "        return self.tp / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate():\n",
    "    model.eval()\n",
    "    eval_acc = Accuracy()\n",
    "    loss, total_steps = 0., 0\n",
    "    for step, batch in enumerate(pbar:=tqdm(eval_dataloader, leave=False)):\n",
    "        pbar.set_description(f\"doing validation\")\n",
    "        batch = to_gpu(batch)\n",
    "        total_steps += 1\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss += loss_fn(out.logits, batch[\"labels\"])  # you could use out.loss and not shift the dataset\n",
    "        eval_acc.update(out.logits, batch[\"labels\"])\n",
    "    # we log results at the end\n",
    "    wandb.log({\"eval/loss\": loss.item() / total_steps,\n",
    "               \"eval/accuracy\": eval_acc.compute()})\n",
    "    prompt_table(eval_dataset[:config.n_eval_samples], log=True)\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def save_model(model, model_name, models_folder=\"models\", log=False):\n",
    "    \"\"\"Save the model to wandb as an artifact\n",
    "    Args:\n",
    "        model (nn.Module): Model to save.\n",
    "        model_name (str): Name of the model.\n",
    "        models_folder (str, optional): Folder to save the model. Defaults to \"models\".\n",
    "    \"\"\"\n",
    "    model_name = f\"{wandb.run.id}_{model_name}\"\n",
    "    file_name = Path(f\"{models_folder}/{model_name}\")\n",
    "    file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "    model.save_pretrained(file_name, safe_serialization=True)\n",
    "    # save tokenizer for easy inference\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model.name_or_path)\n",
    "    tokenizer.save_pretrained(model_name)\n",
    "    if log:\n",
    "        at = wandb.Artifact(model_name, type=\"model\")\n",
    "        at.add_dir(file_name)\n",
    "        wandb.log_artifact(at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:6u6y1cz1) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037fa91df7bb44178e0399c96845fe4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sleek-star-9</strong> at: <a href='https://wandb.ai/warik21/VideoFinder/runs/6u6y1cz1' target=\"_blank\">https://wandb.ai/warik21/VideoFinder/runs/6u6y1cz1</a><br/> View project at: <a href='https://wandb.ai/warik21/VideoFinder' target=\"_blank\">https://wandb.ai/warik21/VideoFinder</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240507_180828-6u6y1cz1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:6u6y1cz1). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f667efe012934c6ea599018893471d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\eriki\\OneDrive\\Documents\\all_folder\\other_projects\\VideoFinder\\wandb\\run-20240507_180857-x6tc71qs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/warik21/alpaca_ft/runs/x6tc71qs' target=\"_blank\">worldly-voice-4</a></strong> to <a href='https://wandb.ai/warik21/alpaca_ft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/warik21/alpaca_ft' target=\"_blank\">https://wandb.ai/warik21/alpaca_ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/warik21/alpaca_ft/runs/x6tc71qs' target=\"_blank\">https://wandb.ai/warik21/alpaca_ft/runs/x6tc71qs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863ee7180ce04684a127da5ae3798113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5055fd348224c428d8658b1fa1f1873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eriki\\OneDrive\\Documents\\all_folder\\other_projects\\VideoFinder\\VFenv\\Lib\\site-packages\\transformers\\models\\gemma\\modeling_gemma.py:573: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1000.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 11.39 GiB is allocated by PyTorch, and 35.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16):\n\u001b[0;32m     14\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m---> 15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m config\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps  \u001b[38;5;66;03m# you could use out.loss and not shift the dataset  \u001b[39;00m\n\u001b[0;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step\u001b[38;5;241m%\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# we can log the metrics to W&B\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[37], line 14\u001b[0m, in \u001b[0;36mloss_fn\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(x, y):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA Flat CrossEntropy\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\eriki\\OneDrive\\Documents\\all_folder\\other_projects\\VideoFinder\\VFenv\\Lib\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1000.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 11.39 GiB is allocated by PyTorch, and 35.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"alpaca_ft\", # the project I am working on\n",
    "           tags=[\"baseline\",\"7b\"],\n",
    "           job_type=\"train\",\n",
    "           config=config) # the Hyperparameters I want to keep track of\n",
    "\n",
    "# Training\n",
    "acc = Accuracy()\n",
    "model.train()\n",
    "train_step = 0\n",
    "for epoch in tqdm(range(config.epochs)):\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = to_gpu(batch)\n",
    "        with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "            out = model(**batch)\n",
    "            loss = loss_fn(out.logits, batch[\"labels\"]) / config.gradient_accumulation_steps  # you could use out.loss and not shift the dataset  \n",
    "            loss.backward()\n",
    "        if step%config.gradient_accumulation_steps == 0:\n",
    "            # we can log the metrics to W&B\n",
    "            wandb.log({\"train/loss\": loss.item() * config.gradient_accumulation_steps,\n",
    "                       \"train/accuracy\": acc.update(out.logits, batch[\"labels\"]),\n",
    "                       \"train/learning_rate\": scheduler.get_last_lr()[0],\n",
    "                       \"train/global_step\": train_step})\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            train_step += 1\n",
    "    validate()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VFenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
